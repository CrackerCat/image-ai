# 中文StableDiffusion-文本生成图像-通用领域

中文Stable Diffusion文生图模型, 输入描述文本，返回符合文本描述的2D图像。

--split--

## 模型描述

本模型采用的是[Stable Diffusion 2.1模型框架](https://github.com/Stability-AI/stablediffusion)，将原始英文领域的[OpenCLIP-ViT/H](https://github.com/mlfoundations/open_clip)文本编码器替换为中文CLIP文本编码器[chinese-clip-vit-huge-patch14](https://github.com/OFA-Sys/Chinese-CLIP)，并使用大规模中文图文pair数据进行训练。训练过程中，固定中文CLIP文本编码器，利用原始Stable Diffusion 2.1 权重对UNet网络参数进行初始化、利用64卡A100共训练35W steps。

## 期望模型使用方式以及适用范围

模型适用范围较广，能基于任意中文文本描述进行推理，生成图像。


### 模型局限性以及可能的偏差

* 模型基于公开数据集及互联网数据进行训练，生成结果可能会存在与训练数据分布相关的偏差。
* 该模型无法实现完美的照片级生成。
* 该模型无法生成清晰的文本。
* 该模型在复杂的组合性生成任务上表现有待提升。


## 训练数据介绍

训练数据包括经中文翻译的公开数据集（LAION-400M、cc12m、Open Images）、以及互联网搜集数据，经过美学得分、图文相关性等预处理进行图像过滤，共计约4亿图文对。